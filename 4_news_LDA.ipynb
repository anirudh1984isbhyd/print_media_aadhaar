{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## note that active directory is already set in thfunction_file_news_corpus\n",
    "## Keep all files in the same active dir\n",
    "## refer to line 48 and line 49 of the function file\n",
    "## pip install --upgrade gensim ( if not installed already )\n",
    "## pip install lda (if not installed already )\n",
    "## pip install pyldavis (if not installed already )\n",
    "## pip install tqdm (if not installed already )\n",
    "## conda install scikit-learn (if not installed already )\n",
    "## LDA is built on top of gensim\n",
    "\n",
    "from function_file_news_corpus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## apply LDA to list of num topics = [5, 10, 15, 20,21, 22, 23, 24, 25 , 26 , 27, 28,29, 30, 32, 34, 36, 38, 40]\n",
    "## store results and check for topics for each\n",
    "## dict dfs stores the topic file for each num topic and also storesa data frame Lda_df, that is a replica\n",
    "## of the original data frame with a new column called 'topic' \n",
    "## with a range [0 : num_topic] for num_topic in count_topics\n",
    "## set _top_topic words as n_top_words\n",
    "\n",
    "news_df = pd.read_pickle('final_clean_hindu_corpus.pkl')\n",
    "news_df.drop(['DateTime'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer(max_df = 0.4, min_df=0.01, max_features=10000, tokenizer=tokenizer_tf_idf,\n",
    "                              stop_words= stopwords(), ngram_range=(1,1))\n",
    "cvz = cvectorizer.fit_transform(news_df['text'])\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "import lda\n",
    "import logging\n",
    "import pyLDAvis\n",
    "\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)\n",
    "\n",
    "count_topics = [5, 10, 15, 20,21, 22, 23, 24, 25 , 26 , 27, 28,29, 30, 32, 34, 36, 38, 40]\n",
    "dict_dfs = []\n",
    "list_dfs = []\n",
    "for n_topics in count_topics:\n",
    "    name_0 = './pyldadavis_v7' + str(n_topics) \n",
    "    name = name_0 + '.html'\n",
    "    name_csv = name_0 +'.csv'\n",
    "    print('n_topics = ', n_topics )\n",
    "    n_iter = 3000\n",
    "    lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "    n_top_words = 20\n",
    "    topic_summaries = []\n",
    "\n",
    "    topic_word = lda_model.topic_word_  # get the topic words\n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "    topic_file = []\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        topic_summaries.append(' '.join(topic_words))\n",
    "        top = 'Topic {}: {}'.format(i, ' '.join(topic_words))\n",
    "        topic_file.append(dict(topic = i, topic_name = top))\n",
    "\n",
    "    topic_file = pd.DataFrame(topic_file)\n",
    "\n",
    "    topic_file.to_csv(name_csv)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "    tsne_lda = tsne_model.fit_transform(X_topics)\n",
    "    doc_topic = lda_model.doc_topic_\n",
    "    lda_keys = []\n",
    "\n",
    "    for i, tweet in enumerate(news_df['text']):\n",
    "        lda_keys += [doc_topic[i].argmax()]\n",
    "\n",
    "\n",
    "    lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "    lda_df['doct_no'] = news_df['doct_no']\n",
    "    lda_df['topic'] = lda_keys\n",
    "    lda_df['text'] = news_df['text']\n",
    "    lda_df['original_text'] = news_df['original_text']\n",
    "    lda_df['date'] = news_df['num_date']\n",
    "\n",
    "    #Preparing Lda_df\n",
    "    news_df['tokens'] = news_df['text'].map(tokenizer_tf_idf)\n",
    "    lda_df['len_docs'] = news_df['tokens'].map(len)\n",
    "    list_dfs.append(lda_df)\n",
    "    dict_dfs.append(dict(num_topic =n_topics,  Lda_df = lda_df, topics = topic_file))\n",
    "\n",
    "    def prepareLDAData():\n",
    "        data = {\n",
    "            'vocab': vocab,\n",
    "            'doc_topic_dists': lda_model.doc_topic_,\n",
    "            'doc_lengths': list(lda_df['len_docs']),\n",
    "            'term_frequency':cvectorizer.vocabulary_,\n",
    "            'topic_term_dists': lda_model.components_\n",
    "        } \n",
    "        return data\n",
    "    ldadata = prepareLDAData()\n",
    "\n",
    "    import pyLDAvis\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    prepared_data = pyLDAvis.prepare(**ldadata)\n",
    "    pyLDAvis.save_html(prepared_data,name)\n",
    "\n",
    "\n",
    "    print('############################################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run LDA again specifically for num topics = 29 \n",
    "## post results in the file given below in the active directory \n",
    "\n",
    "\n",
    "news_df = pd.read_pickle('final_clean_hindu_corpus.pkl')\n",
    "news_df.drop(['DateTime'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "news_df.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer(max_df = 0.4, min_df=0.01, max_features=10000, tokenizer=tokenizer_tf_idf,\n",
    "                              stop_words= stopwords(), ngram_range=(1,1))\n",
    "cvz = cvectorizer.fit_transform(news_df['text'])\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "import lda\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)\n",
    "\n",
    "count_topics = [29]\n",
    "dict_dfs = []\n",
    "list_dfs = []\n",
    "for n_topics in count_topics:\n",
    "    name_0 = './pyldadavis_v7' + str(n_topics) \n",
    "    name = name_0 + '.html'\n",
    "    name_csv = name_0 +'.csv'\n",
    "    print('n_topics = ', n_topics )\n",
    "    n_iter = 3000\n",
    "    lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "    n_top_words = 20\n",
    "    topic_summaries = []\n",
    "\n",
    "    topic_word = lda_model.topic_word_  # get the topic words\n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "    topic_file = []\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        topic_summaries.append(' '.join(topic_words))\n",
    "        top = 'Topic {}: {}'.format(i, ' '.join(topic_words))\n",
    "        topic_file.append(dict(topic = i, topic_name = top))\n",
    "\n",
    "    topic_file = pd.DataFrame(topic_file)\n",
    "\n",
    "    topic_file.to_csv(name_csv)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "    tsne_lda = tsne_model.fit_transform(X_topics)\n",
    "    doc_topic = lda_model.doc_topic_\n",
    "    lda_keys = []\n",
    "\n",
    "    for i, tweet in enumerate(news_df['text']):\n",
    "        lda_keys += [doc_topic[i].argmax()]\n",
    "\n",
    "\n",
    "    lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "    lda_df['doct_no'] = news_df['doct_no']\n",
    "    lda_df['topic'] = lda_keys\n",
    "    lda_df['text'] = news_df['text']\n",
    "    lda_df['original_text'] = news_df['original_text']\n",
    "    lda_df['date'] = news_df['num_date']\n",
    "\n",
    "    #Preparing Lda_df\n",
    "    news_df['tokens'] = news_df['text'].map(tokenizer_tf_idf)\n",
    "    lda_df['len_docs'] = news_df['tokens'].map(len)\n",
    "    list_dfs.append(lda_df)\n",
    "    dict_dfs.append(dict(num_topic =n_topics,  Lda_df = lda_df, topics = topic_file))\n",
    "\n",
    "    def prepareLDAData():\n",
    "        data = {\n",
    "            'vocab': vocab,\n",
    "            'doc_topic_dists': lda_model.doc_topic_,\n",
    "            'doc_lengths': list(lda_df['len_docs']),\n",
    "            'term_frequency':cvectorizer.vocabulary_,\n",
    "            'topic_term_dists': lda_model.components_\n",
    "        } \n",
    "        return data\n",
    "    ldadata = prepareLDAData()\n",
    "\n",
    "    import pyLDAvis\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    prepared_data = pyLDAvis.prepare(**ldadata)\n",
    "    pyLDAvis.save_html(prepared_data,name)\n",
    "\n",
    "\n",
    "    print('############################################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save results of LDA with 29 topics for specific analysis\n",
    "\n",
    "news_df_topics_for_sentiment_analysis_final = dict_dfs[0]['Lda_df'].copy(deep=True)\n",
    "news_df_topics_for_sentiment_analysis_final.drop(['x', 'y' , 'len_docs'], axis = 1, inplace = True)\n",
    "news_df_topics_for_sentiment_analysis_final.to_pickle('news_df_topics_for_sentiment_analysis_final.pkl')\n",
    "\n",
    "\n",
    "news_df_topic_key_final = dict_dfs[0]['topics'].copy(deep=True)\n",
    "writer = pd.ExcelWriter('news_df_topic_key_final.xlsx')\n",
    "news_df_topic_key_final.to_excel(writer, sheet_name='topic_key')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
