{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#\tFUNCTION DEF\n",
    "################################################################################################################\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import  Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import lxml\n",
    "import lxml.html\n",
    "import re\n",
    "\n",
    "# here we are generating links of all pages that contain news related to aadhar\n",
    "def get_all_hindu_pages_links(driver):\n",
    "    elem_pgnum=driver.find_element_by_class_name(\"pagination\")\n",
    "    page_elem=elem_pgnum.find_elements_by_xpath(\"./li/a\")[-1]\n",
    "    last_pg_str=page_elem.get_attribute(\"href\")\n",
    "    num=re.findall('\\\\b\\\\d+\\\\b',last_pg_str)\n",
    "    last_pgnum=int(num[0])\n",
    "    pg_num=2\n",
    "    list_links=[]\n",
    "    while(pg_num!=last_pgnum+1):\n",
    "        url= \"http://www.thehindu.com/search/?order=DESC&page=\" + str(pg_num)+\"&q=aadhar&sort=publishdate\";\n",
    "        print(url)\n",
    "        pg_num=pg_num+1;  \n",
    "        list_links.append(url)\n",
    "    return (list_links)\n",
    "\n",
    "# we are extracting all webpages using request so using requests and beautiful soup to get all href of articles\n",
    "def get_hindu_articles_hyperlinks(links,news_articles_links):\n",
    "    for k in range(len(links)):\n",
    "        response1 = requests.get(links[k],headers=header,timeout=200)\n",
    "        if response1.status_code!=200:\n",
    "            print(\"ERROR\"+ str(response1.status_code))\n",
    "        soup = BeautifulSoup(response1.content, 'html.parser')\n",
    "        a1=soup.find_all(\"p\",attrs={'class':'story-card-33-heading'})\n",
    "        for a in a1:\n",
    "            a2=a.find(\"a\",href=True)\n",
    "            news_articles_links.append(a2['href'])\n",
    "\n",
    "    for length1 in range(len(news_articles_links)):\n",
    "    #print(length1)\n",
    "        if((news_articles_links[length1].find(\"migration_catalog\")!=-1)):\n",
    "            del news_articles_links[length1]\n",
    "        if((news_articles_links[length1].find(\"videos\")!=-1)):\n",
    "            del news_articles_links[length1]\n",
    "\n",
    "    return news_articles_links\n",
    "\n",
    "def scrape_all_hindu_news(news_articles_links,hindu_news):\n",
    "\n",
    "    for i in range(len(news_articles_links)):\n",
    "        if(i==121):\n",
    "            continue\n",
    "        elif(i==1520):\n",
    "            continue\n",
    "        else:\n",
    "            response2=requests.get(news_articles_links[i])\n",
    "            #response2.text\n",
    "            soup = BeautifulSoup(response2.content, 'html.parser')\n",
    "\n",
    "            source_link=news_articles_links[i]\n",
    "\n",
    "            #print(source_link)\n",
    "\n",
    "            heading=soup.find('h1')\n",
    "            if heading is not None:\n",
    "                heading=heading.text\n",
    "            else:\n",
    "                heading=\"No Content\"\n",
    "                print(str(i)+\"+No Content HEADING\")\n",
    "                print(source_link)\n",
    "\n",
    "            #print(heading)\n",
    "            desc=soup.find('h2',attrs={'class':'intro'})\n",
    "\n",
    "            if desc is not None:\n",
    "                desc=desc.text\n",
    "            else:\n",
    "                desc=\"No Content\"\n",
    "                #print(str(i)+\"+No Content DESC\")\n",
    "                #print(source_link)\n",
    "\n",
    "            #print(desc)    \n",
    "            datetime_tag=soup.find_all('span',attrs={'class':'blue-color ksl-time-stamp'})   \n",
    "            for t in datetime_tag:\n",
    "                datetime=t.find('none')\n",
    "                #print(type(datetime))\n",
    "            if datetime is not None:\n",
    "                date=datetime.text\n",
    "            else:\n",
    "                date=\"No Content\"\n",
    "                print(str(i)+\"+No Content:Datetime\")\n",
    "                print(source_link)\n",
    "\n",
    "            #print(date)\n",
    "\n",
    "            news_body_tag=soup.find('div',attrs={'class':'article'})\n",
    "            if(news_body_tag):\n",
    "                x=re.findall(\"content-body-\\\\d+-\\\\d+\",str(news_body_tag.contents))\n",
    "                if(x):\n",
    "                    news_body=soup.find('div',attrs={'id':x[0]})\n",
    "                    if news_body is not None:\n",
    "                        news_body=news_body.text\n",
    "                    else:\n",
    "                        news_body=\"No Content\"\n",
    "                        print(str(i)+\"+No Content :BODY\")\n",
    "                        print(source_link)\n",
    "                else:\n",
    "                    news_body=\"No Content\"\n",
    "                    print(str(i)+\"+No Content :BODY\")\n",
    "                    print(source_link)\n",
    "            else:\n",
    "                    news_body=\"No Content\"\n",
    "                    print(str(i)+\"+No Content :BODY\")\n",
    "                    print(source_link)\n",
    "\n",
    "            #print(news_body)\n",
    "\n",
    "            #hindu_articles_500=[]\n",
    "            #hindu_articles_1000=[]\n",
    "            #hindu_articles_1500=[]\n",
    "            #hindu_articles_2000=[]\n",
    "            #hindu_articles_2500=[]\n",
    "            #hindu_articles_3000=[]\n",
    "            #hindu_articles_3500=[]\n",
    "            #hindu_articles_4000=[]   \n",
    "\n",
    "            hindu_news.append(dict(Heading=heading,Description=desc,DateTime=date,SourceLink=\"THEHINDU\",NewsBody=news_body))\n",
    "\n",
    "            counter = [ 500, 1000, 1500 ,2000, 2500, 3000, 3500 ]\n",
    "            if i in counter:\n",
    "                print(i, 'great')\n",
    "\n",
    "    return hindu_news\n",
    "            \n",
    "###############################################################################################################################\n",
    "#\tRUNNING CMDS\n",
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "main_url=\"http://www.thehindu.com/archive/\"\n",
    "# we will add all different archive urls to this main url. At every archive url system will type aadhar and search info link relateds\n",
    "chrome_path=\"D:/5.0 New Setups/chromedriver_win32/chromedriver\"\n",
    "header = {'User-Agent': 'Mozilla/58.0.2 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36'}\n",
    "\n",
    "keyword_list= [\"aadhar\"]\n",
    "driver = webdriver.Chrome(executable_path=chrome_path)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "driver.get(main_url)\n",
    "search_elemnt = driver.find_element_by_name('q')\n",
    "search_elemnt.send_keys(keyword_list[0])\n",
    "search_elemnt.send_keys(Keys.RETURN)\n",
    "\n",
    "# below we are scraping all href tags of every news article from page 1\n",
    "news_articles_links=[]\n",
    "\n",
    "elem1=driver.find_elements_by_class_name(\"story-card-33-heading\")\n",
    "for e in elem1:\n",
    "    e1=e.find_element_by_xpath(\"./a[@title]\")\n",
    "    news_articles_links.append(e1.get_attribute(\"href\"))\n",
    "    #print(e1.get_attribute(\"href\"))\n",
    "\n",
    "\n",
    "links=get_all_hindu_pages_links(driver)\n",
    "\n",
    "# we are gng to get links of articlices from all other webpages\n",
    "news_articles_links=get_hindu_articles_hyperlinks(links,news_articles_links)\n",
    "hindu_news=[]\n",
    "#now we create files of indivodual articles\n",
    "hindu_news=scrape_all_hindu_news(news_articles_links,hindu_news)\n",
    "\n",
    "hindu=pd.DataFrame(hindu_news)\n",
    "\n",
    "hindu.to_csv('hindu_news.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
