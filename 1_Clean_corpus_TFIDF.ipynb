{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## note that active directory is already set in thfunction_file_news_corpus\n",
    "## Keep all files in the same active dir\n",
    "## refer to line 48 and line 49 of the function file\n",
    "from function_file_news_corpus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## calling function stop places \n",
    "## stopplaces returns a list of user defined place junk to be eliminated from the start of articles\n",
    "stop_places = stopplaces()\n",
    "\n",
    "## calling function stop words \n",
    "## stopwords returns a list of NLTK stopwords + user defined stopwords\n",
    "\n",
    "stop_words = stopwords()\n",
    "\n",
    "## hindu_df.pkl is the webscraped file in pkl format.\n",
    "## hindu_df is a data frame with columns NewsBody, DateTime, Heading\n",
    "## these columns are foreced to be type cast to string format below\n",
    "## We then drop dupliacated if they exist\n",
    "\n",
    "news_df = pd.read_pickle('hindu_df.pkl')\n",
    "\n",
    "news_df.NewsBody = news_df.NewsBody.astype(str)\n",
    "news_df.DateTime = news_df.DateTime.astype(str)\n",
    "news_df.Heading = news_df.Heading.astype(str)\n",
    "news_df.drop_duplicates(inplace = True)\n",
    "print(len(news_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Next, we apply the function clean_corpus_1.\n",
    "## refer to the function file line 57\n",
    "## Rename NewsBody column as text\n",
    "## Clean other columns using the function clean_non_news_body_cols\n",
    "## refer to function file line 452\n",
    "\n",
    "\n",
    "news_df['NewsBody'] = news_df['NewsBody'].apply(clean_corpus_1)\n",
    "news_df = news_df[news_df['NewsBody']!= 0]\n",
    "news_df.rename(columns = {'NewsBody':'text'}, inplace = True)\n",
    "news_df['original_text'] = news_df['text']\n",
    "news_df['DateTime'] = news_df['DateTime'].apply(clean_non_news_body_cols)\n",
    "news_df['Heading'] = news_df['Heading'].apply(clean_non_news_body_cols)\n",
    "\n",
    "\n",
    "## Next, we apply the function clean_corpus_2.\n",
    "## eliminates dangling places ( using stopplaces )\n",
    "## add a column of document no as doct-no \n",
    "\n",
    "news_df['text'] = news_df['text'].apply(clean_corpus_2,stop_places = stop_places )\n",
    "news_df['doct_no'] = range(len(news_df))\n",
    "news_df['doct_no'] = news_df['doct_no'] +1\n",
    "news_df.head()\n",
    "\n",
    "\n",
    "## Next, we apply the function clean_corpus_3.\n",
    "## eliminates stopwords ( using stopwords )\n",
    "## lemmatizes words using POS tagging\n",
    "\n",
    "news_df['text'] = news_df.text.astype(str)\n",
    "news_df['text'] = news_df['text'].apply(clean_corpus_3,stop_words = stop_words)\n",
    "\n",
    "\n",
    "## Next, we further use the function clean_corpus_4.\n",
    "## convert high frequency adverbs / adjectives in text to their root verb / noun \n",
    "news_df['text'] = news_df['text'].apply(clean_corpus_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## We then read text specific bigrams and trigrams  \n",
    "## We eliminate unigrams that co occur as birgrams and trigrams  \n",
    "## Replace them with the  bigrams and trigrams in function clean_corpus_5_replace_unigrams_with_bi_grams\n",
    "\n",
    "bi_gram_lis = pd.read_excel('bigrams_1.xlsx').Bigrams.tolist()\n",
    "tri_gram_lis = pd.read_excel('trigrams_1.xlsx').Trigrams.tolist()\n",
    "\n",
    "news_df['text'] = news_df['text'].apply(clean_corpus_5_replace_unigrams_with_bi_grams, \n",
    "                                            bi_gram_lis = bi_gram_lis, tri_gram_lis = tri_gram_lis)\n",
    "\n",
    "bi_gram_lis = pd.read_excel('bigrams_2.xlsx').Bigrams.tolist()\n",
    "tri_gram_lis = pd.read_excel('trigrams_2.xlsx').Trigrams.tolist()\n",
    "\n",
    "news_df['text'] = news_df['text'].apply(clean_corpus_5_replace_unigrams_with_bi_grams, \n",
    "                                            bi_gram_lis = bi_gram_lis, tri_gram_lis = tri_gram_lis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check for the token  count ' aadhaar' in each article\n",
    "## Do not use birgrmas or unigrams\n",
    "## simply apply functin tokenizer_tf_idf (which seprates words on spaces)\n",
    "## eliminate articles where aadhaar is not used as aadhaar but for some other reference \n",
    "## Keep the remaining columns\n",
    "\n",
    "news_df['new_text'] = news_df['text'].map(clean_check)\n",
    "news_df['new_tokens'] = news_df['new_text'].map(tokenizer_tf_idf)\n",
    "news_df['aadhaar_count'] = news_df['new_tokens'].apply(count_token, token = 'aadhaar')\n",
    "len(news_df[news_df['aadhaar_count']== 1])\n",
    "news_df['aadhaar_count'].hist(bins=100)\n",
    "\n",
    "news_df = count_most_common(news_df,'new_tokens', 1)\n",
    "\n",
    "def col_to_str (top_count_col):\n",
    "    for word in top_count_col:\n",
    "        return word\n",
    "    \n",
    "news_df['new_col'] = news_df['top_1_counts'].apply(col_to_str )\n",
    "\n",
    "d_stop =pd.read_excel('hindu_non_relevant_entries.xlsx')\n",
    "d_stop.entires = d_stop.entires.astype(str)\n",
    "\n",
    "news_df['compare'] =  news_df['new_col'].apply(remove_more_junk_hindu_posts,compare_list = d_stop.entires.tolist() )\n",
    "print(len(news_df[news_df['compare']==0]))\n",
    "print('original', len(news_df))\n",
    "news_df = news_df[news_df['compare']==1]\n",
    "print('final', len(news_df))\n",
    "\n",
    "news_df.drop(['new_text', 'new_tokens' , 'top_1_counts', 'new_col','compare' ], axis = 1, inplace = True)\n",
    "news_df.head()\n",
    "\n",
    "\n",
    "news_df.to_pickle('final_clean_hindu_corpus.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## generate TF_IDF\n",
    "## note all relevant trigrams and bigrams have been captured\n",
    "## tfidf inputs are - data frame, the text column, minimum_cut_off ( all words in documents fewer than \n",
    "## minimum cutoff will not be considered), n_gram set (min_n_gram, max_ngram)\n",
    "\n",
    "tfidf = TF_IDF(news_df,'text', 30, (1,1))\n",
    "writer_orig = pd.ExcelWriter('hindu_tfidf_011.xlsx')\n",
    "tfidf.to_excel(writer_orig, sheet_name='hindu_tfidf_010')\n",
    "writer_orig.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read the cleansed file\n",
    "## remove more duplicates using the function clear_more_duplicates\n",
    "## use the function remove_more_stopwords to remove even more stop words at this stage having generated TFIDF\n",
    "## convert the date time into a readable format / usaable format using the function date_time_hindu\n",
    "## save file in pkl format\n",
    "\n",
    "news_df = pd.read_pickle('final_clean_hindu_corpus.pkl')\n",
    "news_df = clear_more_duplicates(news_df, 'text')\n",
    "news_df['text'] = news_df['text'].map(remove_more_stopwords)\n",
    "news_df = date_time_hindu(news_df, 'DateTime')\n",
    "\n",
    "news_df.to_pickle('final_clean_hindu_corpus.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_df = pd.read_pickle('final_clean_hindu_corpus.pkl')\n",
    "news_df.drop(['DateTime'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
